{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# building an MNIST classifier using a convolutional NN in Tensorflow"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importing some libraries, also the mnist dataset is preprocessed in the tensorflow tutorial libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting MNIST_data\\train-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\train-labels-idx1-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-images-idx3-ubyte.gz\n",
      "Extracting MNIST_data\\t10k-labels-idx1-ubyte.gz\n"
     ]
    }
   ],
   "source": [
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets('MNIST_data', one_hot=True)\n",
    "\n",
    "import tensorflow as tf\n",
    "from time import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Defining some functions we will be using later on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# predefined functions to add noise to initial values of weights and biases\n",
    "def weight_variable(shape):\n",
    "    initial = tf.truncated_normal(shape, stddev=0.1)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "def bias_variable(shape):\n",
    "    initial = tf.constant(0.1, shape=shape)\n",
    "    return tf.Variable(initial)\n",
    "\n",
    "\n",
    "# predefined conv and pool layers with stride and padding set\n",
    "def conv2d(x, W): \n",
    "    return tf.nn.conv2d(x, filter=W, strides=[1, 1, 1, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def max_pool_2x2(x):\n",
    "    return tf.nn.max_pool(x, ksize=[1, 2, 2, 1], strides=[1, 2, 2, 1], padding='SAME')\n",
    "\n",
    "\n",
    "def validation_accuracy(loaded_mnist, accuracy, x, y_, batch_size=64):\n",
    "    \"\"\" A function that calculates the accuracy on the validation data.\"\"\"\n",
    "    batch_num = int(loaded_mnist.test.num_examples / batch_size)\n",
    "    test_accuracy = 0\n",
    "    \n",
    "    for i in range(batch_num):\n",
    "        batch = loaded_mnist.test.next_batch(batch_size)\n",
    "        test_accuracy += accuracy.eval(feed_dict={x: batch[0],\n",
    "                                                  y_: batch[1]})\n",
    "\n",
    "    test_accuracy /= batch_num\n",
    "    return test_accuracy\n",
    "\n",
    "\n",
    "def countParameters():\n",
    "    \"\"\" Counts all the trainable parameters in the current graph. Used for task 3.\"\"\"\n",
    "    total = 0;\n",
    "    for variable in tf.trainable_variables():\n",
    "        shape = variable.get_shape()\n",
    "        variable_parameters = 1\n",
    "        for dimension in shape:\n",
    "            variable_parameters *= dimension.value\n",
    "        total += variable_parameters\n",
    "    return total"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now build the actual convolutional net in a way that enablers us to quickly change and test different hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def convNN(x, dev, num_filters=16):\n",
    "    \"\"\"Returns the output-graph for an convultional NN with following architecture:\n",
    "    2 convolutional layers with num_filters filters, each a size of 3x3 and stride of 1.\n",
    "    Relu activations, max pooling and a fully connected layer at the end with softmax cross\n",
    "    entropy loss calculation.\"\"\"\n",
    "    with tf.device(dev):\n",
    "        num_filters_const = tf.constant(num_filters)\n",
    "    \n",
    "    # reshape images\n",
    "    with tf.device(dev), tf.name_scope('reshape'):\n",
    "        x_image = tf.reshape(x, [-1, 28, 28, 1])\n",
    "\n",
    "\n",
    "    # applying convolution, bias and activation\n",
    "    with tf.device(dev), tf.name_scope('conv1'):\n",
    "        W_conv1 = weight_variable([3, 3, 1, num_filters_const])\n",
    "        b_conv1 = bias_variable([num_filters])\n",
    "        h_conv1 = tf.nn.relu(conv2d(x_image, W_conv1) + b_conv1)\n",
    "\n",
    "\n",
    "    # max pool layer 2\n",
    "    with tf.device(dev), tf.name_scope('pool2'):\n",
    "        h_pool1 = max_pool_2x2(h_conv1)\n",
    "\n",
    "\n",
    "    # applying convolution, bias and activation\n",
    "    with tf.device(dev), tf.name_scope('conv2'):\n",
    "        W_conv2 = weight_variable([3, 3, num_filters_const, num_filters_const])\n",
    "        b_conv2 = bias_variable([num_filters])\n",
    "        h_conv2 = tf.nn.relu(conv2d(h_pool1, W_conv2) + b_conv2)\n",
    "        \n",
    "    # max pool layer 2\n",
    "    with tf.device(dev), tf.name_scope('pool2'):\n",
    "        h_pool2 = max_pool_2x2(h_conv2)\n",
    "\n",
    "\n",
    "\n",
    "    # flatten input and feed forward\n",
    "    with tf.device(dev), tf.name_scope('fcl1'):\n",
    "        # fully connected layer\n",
    "        W_fc1 = weight_variable([7*7*num_filters, 128])\n",
    "        b_fc1 = bias_variable([128])\n",
    "        h_pool2_flat = tf.reshape(h_pool2, [-1, 7*7*num_filters])\n",
    "        h_fc1 = tf.nn.relu(tf.matmul(h_pool2_flat, W_fc1) + b_fc1)\n",
    "\n",
    "\n",
    "    # readout layer, fully connected\n",
    "    with tf.device(dev), tf.name_scope('fcl2'):\n",
    "        W_fc2 = weight_variable([128, 10])\n",
    "        b_fc2 = bias_variable([10])\n",
    "        y_conv = tf.matmul(h_fc1, W_fc2) + b_fc2\n",
    "        \n",
    "    return y_conv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With all these tools, we can now build an easy way to run tests and check nets with different parameters with just one function call."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def testArchitecture(learning_rate=0.1, num_filters=16, device='gpu'):\n",
    "    \"\"\" \n",
    "    Builds a conv NN with the given parameters and trains it, \n",
    "    calculating the number of trainable parameters and measuring \n",
    "    the runtime as well as the performance during training.\n",
    "    \"\"\"\n",
    "    \n",
    "    dev = '/GPU:0' if device == 'gpu' else '/cpu:0'\n",
    "    print('Starting to train a convolutional NN with {} filters, learning rate {}, using the {}.'.format(num_filters, learning_rate, device))\n",
    "    \n",
    "    with tf.device(dev):\n",
    "        # Placeholder for input\n",
    "        x = tf.placeholder(tf.float32, [None, 784])\n",
    "\n",
    "        # Placeholder for predicted output\n",
    "        y_ = tf.placeholder(tf.float32, [None, 10])\n",
    "    \n",
    "    # build a net with the given parameters\n",
    "    y = convNN(x, dev, num_filters)\n",
    "    \n",
    "    with tf.device(dev), tf.name_scope('loss'):\n",
    "        cross_entropy = tf.nn.softmax_cross_entropy_with_logits(labels=y_, logits=y)\n",
    "        cross_entropy = tf.reduce_mean(cross_entropy)\n",
    "    tf.summary.scalar('cross_entropy', cross_entropy)\n",
    "    \n",
    "    with tf.device(dev), tf.name_scope(\"SGD\"):\n",
    "        train_step = tf.train.GradientDescentOptimizer(learning_rate).minimize(cross_entropy)\n",
    "        \n",
    "    with tf.device(dev), tf.name_scope('accuracy'):\n",
    "        correct_prediction = tf.equal(tf.argmax(y, 1), tf.argmax(y_, 1))\n",
    "        correct_prediction = tf.cast(correct_prediction, tf.float32)\n",
    "        accuracy = tf.reduce_mean(correct_prediction)\n",
    "        \n",
    "    tf.summary.scalar('accuracy', accuracy)\n",
    "    merged_summary_op = tf.summary.merge_all()\n",
    "    \n",
    "    with tf.device(dev), tf.Session(config=tf.ConfigProto(log_device_placement=True)) as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "        writer = tf.summary.FileWriter('C:/Users/Kai/Desktop/Logs/' \n",
    "                                       + str(learning_rate)\n",
    "                                       + '/' + str(num_filters)\n",
    "                                       + '/' + device, graph=sess.graph)\n",
    "        start_time = time()\n",
    "        print(\"trainable parameters: \" + str(countParameters()))\n",
    "        for epoch in range(2000):\n",
    "            batch = mnist.train.next_batch(50)\n",
    "            batch_val = mnist.validation.next_batch(50)\n",
    "            if epoch % 500 == 0:\n",
    "                val_accuracy = validation_accuracy(mnist, accuracy, x, y_)\n",
    "                print('step {}, validation accuracy {:.2f}%'.format(epoch, val_accuracy*100), end='\\n')\n",
    "    \n",
    "            with tf.device(dev):\n",
    "                train_step.run(feed_dict={x: batch[0], y_: batch[1]})\n",
    "\n",
    "            # collect data for summary\n",
    "            summary_str = sess.run(merged_summary_op, feed_dict={x: batch[0], y_: batch[1]})\n",
    "            writer.add_summary(summary_str, epoch)\n",
    "        print(\"time needed: \" + str(time() - start_time))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Thats it, let's check out some different parameters!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting to train a convolutional NN with 16 filters, learning rate 0.1, using the gpu.\n",
      "trainable parameters: 104250\n",
      "step 0, validation accuracy 8.18%\n",
      "step 500, validation accuracy 96.04%\n",
      "step 1000, validation accuracy 97.40%\n",
      "step 1500, validation accuracy 98.05%\n",
      "time needed: 35.28899097442627\n",
      "Starting to train a convolutional NN with 16 filters, learning rate 0.01, using the gpu.\n",
      "trainable parameters: 104250\n",
      "step 0, validation accuracy 10.09%\n",
      "step 500, validation accuracy 86.67%\n",
      "step 1000, validation accuracy 91.49%\n",
      "step 1500, validation accuracy 93.18%\n",
      "time needed: 32.38352966308594\n",
      "Starting to train a convolutional NN with 16 filters, learning rate 0.001, using the gpu.\n",
      "trainable parameters: 104250\n",
      "step 0, validation accuracy 11.87%\n",
      "step 500, validation accuracy 18.30%\n",
      "step 1000, validation accuracy 36.12%\n",
      "step 1500, validation accuracy 49.51%\n",
      "time needed: 33.02908277511597\n",
      "Starting to train a convolutional NN with 16 filters, learning rate 0.0001, using the gpu.\n",
      "trainable parameters: 104250\n",
      "step 0, validation accuracy 9.97%\n",
      "step 500, validation accuracy 11.38%\n",
      "step 1000, validation accuracy 11.56%\n",
      "step 1500, validation accuracy 11.74%\n",
      "time needed: 35.73034119606018\n",
      "Starting to train a convolutional NN with 8 filters, learning rate 0.1, using the gpu.\n",
      "trainable parameters: 52258\n",
      "step 0, validation accuracy 6.46%\n",
      "step 500, validation accuracy 94.82%\n",
      "step 1000, validation accuracy 96.77%\n",
      "step 1500, validation accuracy 97.38%\n",
      "time needed: 25.192660331726074\n",
      "Starting to train a convolutional NN with 16 filters, learning rate 0.1, using the gpu.\n",
      "trainable parameters: 104250\n",
      "step 0, validation accuracy 10.02%\n",
      "step 500, validation accuracy 95.95%\n",
      "step 1000, validation accuracy 97.68%\n",
      "step 1500, validation accuracy 97.86%\n",
      "time needed: 32.434271812438965\n",
      "Starting to train a convolutional NN with 32 filters, learning rate 0.1, using the gpu.\n",
      "trainable parameters: 211690\n",
      "step 0, validation accuracy 9.73%\n",
      "step 500, validation accuracy 96.74%\n",
      "step 1000, validation accuracy 97.80%\n",
      "step 1500, validation accuracy 98.26%\n",
      "time needed: 51.30483150482178\n",
      "Starting to train a convolutional NN with 64 filters, learning rate 0.1, using the gpu.\n",
      "trainable parameters: 440394\n",
      "step 0, validation accuracy 5.11%\n",
      "step 500, validation accuracy 97.07%\n",
      "step 1000, validation accuracy 97.58%\n",
      "step 1500, validation accuracy 98.25%\n",
      "time needed: 93.32016062736511\n",
      "Starting to train a convolutional NN with 128 filters, learning rate 0.1, using the gpu.\n",
      "trainable parameters: 953098\n",
      "step 0, validation accuracy 9.64%\n",
      "step 500, validation accuracy 96.39%\n",
      "step 1000, validation accuracy 97.48%\n",
      "step 1500, validation accuracy 98.16%\n",
      "time needed: 198.74890542030334\n",
      "Starting to train a convolutional NN with 256 filters, learning rate 0.1, using the gpu.\n",
      "trainable parameters: 2199690\n",
      "step 0, validation accuracy 9.55%\n",
      "step 500, validation accuracy 95.92%\n",
      "step 1000, validation accuracy 96.78%\n",
      "step 1500, validation accuracy 96.54%\n",
      "time needed: 449.803014755249\n",
      "Starting to train a convolutional NN with 8 filters, learning rate 0.1, using the cpu.\n",
      "trainable parameters: 52258\n",
      "step 0, validation accuracy 20.32%\n",
      "step 500, validation accuracy 95.59%\n",
      "step 1000, validation accuracy 96.66%\n",
      "step 1500, validation accuracy 97.51%\n",
      "time needed: 78.93962526321411\n",
      "Starting to train a convolutional NN with 16 filters, learning rate 0.1, using the cpu.\n",
      "trainable parameters: 104250\n",
      "step 0, validation accuracy 11.02%\n",
      "step 500, validation accuracy 95.83%\n",
      "step 1000, validation accuracy 97.69%\n",
      "step 1500, validation accuracy 97.91%\n",
      "time needed: 108.73169445991516\n",
      "Starting to train a convolutional NN with 32 filters, learning rate 0.1, using the cpu.\n",
      "trainable parameters: 211690\n",
      "step 0, validation accuracy 9.95%\n",
      "step 500, validation accuracy 95.72%\n",
      "step 1000, validation accuracy 97.69%\n",
      "step 1500, validation accuracy 98.27%\n",
      "time needed: 208.90988302230835\n",
      "Starting to train a convolutional NN with 64 filters, learning rate 0.1, using the cpu.\n",
      "trainable parameters: 440394\n",
      "step 0, validation accuracy 10.40%\n",
      "step 500, validation accuracy 96.42%\n",
      "step 1000, validation accuracy 97.94%\n",
      "step 1500, validation accuracy 98.30%\n",
      "time needed: 461.2800860404968\n"
     ]
    }
   ],
   "source": [
    "for testing_learning_rate in [0.1, 0.01, 0.001, 0.0001]:\n",
    "    testArchitecture(learning_rate=testing_learning_rate, num_filters=16, device='gpu')\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "for number_of_filters in [8, 16, 32, 64, 128, 256]:\n",
    "    testArchitecture(learning_rate=0.1, num_filters=number_of_filters, device='gpu')\n",
    "    tf.reset_default_graph()\n",
    "\n",
    "    \n",
    "for number_of_filters in [8, 16, 32, 64]:\n",
    "    testArchitecture(learning_rate=0.1, num_filters=number_of_filters, device='cpu')\n",
    "    tf.reset_default_graph()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
